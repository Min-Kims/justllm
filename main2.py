import streamlit as st
#êµ¬ë™í•œ ê²ƒì´ Streamlit ìƒ ë¡œê·¸ë¡œ ë‚¨ë„ë¡
from loguru import logger
#ë©”ëª¨ë¦¬ë¥¼ ê°€ì§€ê³  ìžˆëŠ” Conver~~~Chain ê°€ì ¸ì˜¤ê¸°
from langchain.chains import ConversationalRetrievalChain
#LLM : OpenAI
from langchain.chat_models import ChatOpenAI
#ëª‡ê°œê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ë©”ëª¨ë¦¬ë¡œ ë„£ì–´ì¤„ì§€ ì •í•˜ëŠ” ë¶€ë¶„
from langchain.memory import ConversationBufferMemory
#ë©”ëª¨ë¦¬ êµ¬ì¶•ì„ ìœ„í•œ ì¶”ê°€ì  ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.callbacks import get_openai_callback
from langchain.memory import StreamlitChatMessageHistory
from dotenv import load_dotenv

def main():
    st.set_page_config(page_title="ê¹€ë¯¼ ì±—ë´‡ ë°ëª¨", page_icon="ðŸ¦œ")
    st.title("ðŸ¦œ ê¹€ë¯¼ ì±—ë´‡ ë°ëª¨")

        if "conversation" not in st.session_state:
            st.session_state.conversation = None

        if "chat_history" not in st.session_state:
            st.session_state.chat_history = None

        if "processComplete" not in st.session_state:
            st.session_state.processComplete = None

        with st.sidebar:
            process = st.button("Process")

        if process:
            #Streamlit Secretí‚¤ ê°€ì ¸ì˜¤ê¸°
            openai_api_key = st.secrets["openai"]["api_key"]
            if not openai_api_key:
                st.info("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. secret.tomlíŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.stop()

        st.session_state.conversation = get_conversation_chain(openai_api_key) 

        st.session_state.processComplete = True
    
    #ì±„íŒ…ì°½ì˜ ë©”ì„¸ì§€ë“¤ì´ ê³„ì† ìœ ì§€ë˜ê¸° ìœ„í•œ ì½”ë“œ
        if 'messages' not in st.session_state:
            st.session_state['messages'] = [{"role": "assistant", 
                                            "content": "ì•ˆë…•í•˜ì„¸ìš”! ì£¼ì–´ì§„ ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ê²ƒì´ ìžˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!"}]

    #ë©”ì„¸ì§€ë“¤ë§ˆë‹¤ Roleì— ë”°ë¼ ë§ˆí¬ë‹¤ìš´ í• ê²ƒì´ë‹¤. (í•œ ë²ˆ ë©”ì„¸ì§€ê°€ ìž…ë ¥ë  ë•Œë§ˆë‹¤ ë¬¶ëŠ” ê²ƒ)
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])


    #ížˆìŠ¤í† ë¦¬ë¥¼ êµ¬í˜„í•´ì¤˜ì•¼ ìš°ë¦¬ì˜ ì§ˆë¬¸ì„ ê¸°ì–µí•´ì„œ ëŒ‡ë‹µí•´ì¤Œ
        history = StreamlitChatMessageHistory(key="chat_messages")

    #ì§ˆë¬¸ ì°½ ë§Œë“¤ê¸°
        # Chat logic
        if query := st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”."):
            st.session_state.messages.append({"role": "user", "content": query})

            with st.chat_message("user"):
                st.markdown(query)

            with st.chat_message("assistant"):

                chain = st.session_state.conversation

    #ë¡œë”©í•  ë•Œ ë¹™ê¸€ë¹™ê¸€ ëŒì•„ê°€ëŠ” ì˜ì—­ êµ¬í˜„
                with st.spinner("Thinking..."):
                    try:
                        result = chain({"question": query})
                        with get_openai_callback() as cb:
                            st.session_state.chat_history = result.get('chat_history', [])
                        response = result.get('answer', "ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        source_documents = result.get('source_documents', [])

                        st.markdown(response)
                        if source_documents:
                            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                                for doc in source_documents:
                                    st.markdown(doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ'), help=doc.page_content)
                    except Exception as e:
                        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # Add assistant message to chat history
            st.session_state.messages.append({"role": "assistant", "content": response})                    


#ìœ„ì—ì„œ ì„ ì–¸í•œ ëª¨ë“ ê²ƒë“¤ì„ ë‹´ì•„ë³´ëŠ” ê²ƒ
def get_conversation_chain(openai_api_key):
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name = 'gpt-3.5-turbo',temperature=0)
    conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=llm, 
            chain_type="stuff", 
            memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),
            get_chat_history=lambda h: h,
            return_source_documents=True,
        )

    return conversation_chain

if __name__ == '__main__':
    main()  







# class StreamHandler(BaseCallbackHandler):
#     def __init__(self, container, initial_text=""):
#         self.container = container
#         self.text = initial_text

#     def on_llm_new_token(self, token: str, **kwargs) -> None:
#         self.text += token
#         self.container.markdown(self.text)

